# 接口设计说明书

---

项目名称：HVAC故障预测与检测系统

版本：1.0

日期：2024年8月

---

## 1. 简介

本接口设计说明书旨在详细描述HVAC故障预测与检测系统的各个接口的功能、使用方法及其内部实现逻辑。通过阅读本说明书，开发者能够了解系统如何与外部数据源交互、如何进行数据处理、以及如何输出故障检测与预测的结果。

## 2. 系统架构概述

HVAC故障预测与检测系统的架构主要由以下组件组成：
- **数据源**：包括实时传感器数据及仿真生成的数据。
- **数据流管理**：由Kafka负责数据的实时传输和管理。
- **数据处理与存储**：基于Spark和Hadoop的分布式处理与存储。
- **核心算法**：基于TimeMixer的故障预测与检测算法。

## 3. 接口设计原则

在设计接口时，我们遵循以下原则：
- **模块化**：各个接口相互独立，便于维护和扩展。
- **易用性**：接口设计简单明了，提供详细的参数说明和使用示例。
- **可扩展性**：接口具备良好的扩展性，支持未来的功能扩展。
- **安全性**：所有接口均需考虑数据的安全性，防止未经授权的访问。
- **性能优化**：接口实现需考虑系统的性能，避免出现瓶颈。

## 4. 接口详细说明

以下是接口设计说明书的详细说明，包含了每个接口的描述、参数、返回值、错误处理及示例代码。

### **1. IoT 设备集成接口**

#### 1.1 `getSensorData(sensorType, sensorId)`
- **描述**: 从指定的传感器获取实时数据。
- **参数**:
  - `sensorType`: 字符串，传感器类型（如温度、湿度、压力等）。
  - `sensorId`: 字符串，传感器的唯一标识符。
- **返回值**:
  - 成功时返回传感器数据。
  - 失败时返回错误信息（如传感器ID无效或传感器类型不匹配）。
- **错误处理**: 
  - 无效的传感器ID将返回`"Error: Invalid sensor ID"`。
  - 传感器类型不匹配将返回`"Error: Sensor type mismatch"`。
  - 异常情况下返回捕获到的错误信息。
- **示例代码**:
    ```python
    sensor_data = getSensorData("temperature", "sensor1")
    print(sensor_data)  # 输出传感器数据或错误信息
    ```

#### 1.2 `configureSensor(sensorType, sensorId, configParams)`
- **描述**: 配置指定传感器的参数。
- **参数**:
  - `sensorType`: 字符串，传感器类型。
  - `sensorId`: 字符串，传感器的唯一标识符。
  - `configParams`: 字典，配置参数。
- **返回值**:
  - 成功时返回`True`。
  - 失败时返回`False`。
- **错误处理**:
  - 无效的传感器ID或类型不匹配返回`False`。
  - 异常情况下返回`False`。
- **示例代码**:
    ```python
    config_success = configureSensor("humidity", "sensor2", {"interval": 120})
    print(config_success)  # 输出配置成功或失败的结果
    ```

### **2. Apache Flume 接口**

#### 2.1 `flumeSourceSetup(sourceType, sourceConfig)`
- **描述**: 设置 Flume 的 Source，指定数据源类型及其配置。
- **参数**:
  
  - `sourceType`: 字符串，数据源类型（如`avro`, `exec`, `log4j`等）。
  - `sourceConfig`: 字典，数据源配置。
- **返回值**:
  - 成功时返回`True`。
  - 失败时返回`False`。
- **错误处理**:
  - 不支持的`sourceType`抛出`FlumeSetupError`并返回`False`。
  - 异常情况下返回`False`。
- **示例代码**:
    ```python
    source_success = flumeSourceSetup("avro", {"host": "localhost", "port": 4141})
    print(source_success)  # 输出设置成功或失败的结果
    ```

#### 2.2 `flumeSinkSetup(sinkType, sinkConfig)`
- **描述**: 设置 Flume 的 Sink，指定目标系统及其配置。
- **参数**:
  - `sinkType`: 字符串，目标系统类型（如`avro`, `hdfs`, `log4j`等）。
  - `sinkConfig`: 字典，目标系统配置。
- **返回值**:
  - 成功时返回`True`。
  - 失败时返回`False`。
- **错误处理**:
  - 不支持的`sinkType`抛出`FlumeSetupError`并返回`False`。
  - 异常情况下返回`False`。
- **示例代码**:
    ```python
    sink_success = flumeSinkSetup("hdfs", {"path": "hdfs://localhost:9000/flume/data"})
    print(sink_success)  # 输出设置成功或失败的结果
    ```

### **3. Apache Kafka 接口**

#### 3.1 `kafkaProduce(topic, message)`
- **描述**: 将消息发送到 Kafka 的指定主题。
- **参数**:
  
  - `topic`: 字符串，Kafka 主题名称。
  - `message`: 字典，发送的消息，必须是 Brick 格式。
- **返回值**:
  - 成功时返回`True`。
  - 失败时返回`False`。
- **错误处理**:
  - 异常情况下返回`False`并打印错误信息。
- **示例代码**:
    ```python
    produce_success = kafkaProduce('test_topic', {'key': 'value'})
    print(produce_success)  # 输出发送成功或失败的结果
    ```

#### 3.2 `kafkaConsume(topic, groupId)`
- **描述**: 从 Kafka 的指定主题中消费消息。
- **参数**:
  - `topic`: 字符串，Kafka 主题名称。
  - `groupId`: 字符串，消费者组 ID。
- **返回值**:
  - 成功时返回消费到的消息。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    consume_message = kafkaConsume('test_topic', 'test_group')
    print(consume_message)  # 输出消费到的消息或错误信息
    ```

### **4. Apache Spark 接口**

#### 4.1 `sparkStreamProcess(kafkaParams, processingFunc)`
- **描述**: 使用 Spark Streaming 处理 Kafka 数据流。
- **参数**:
  - `kafkaParams`: 字典，Kafka 参数。
  - `processingFunc`: 函数，处理函数，用于处理 Brick 格式的数据。
- **返回值**:
  - 成功时返回`True`。
  - 失败时返回`False`。
- **错误处理**:
  - 异常情况下返回`False`并打印错误信息。
- **示例代码**:
    ```python
    kafka_params = {
        'metadata.broker.list': 'localhost:9092',
        'topic': 'test_topic',
        'groupId': 'test_group'
    }
    
    stream_success = sparkStreamProcess(kafka_params, process_brick_data)
    print(stream_success)  # 输出处理成功或失败的结果
    ```

#### 4.2 `sparkSQLQuery(sc, query)`
- **描述**: 执行 Spark SQL 查询。
- **参数**:
  - `sc`: SparkContext 对象。
  - `query`: 字符串，SQL 查询语句。
- **返回值**:
  - 成功时返回查询结果。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    query_result = sparkSQLQuery(sc, "SELECT * FROM brick_table")
    print(query_result)  # 输出查询结果或错误信息
    ```

### **5. Spark MLlib 接口**

#### 5.1 `trainModel(trainingData, modelParams)`
- **描述**: 使用 Spark MLlib 训练预测模型。
- **参数**:
  - `trainingData`: RDD，训练数据（LabeledPoint 格式）。
  - `modelParams`: 字典，模型参数。
- **返回值**:
  - 成功时返回训练好的模型。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    training_rdd = ...
    model_params = {"step": 0.01, "iterations": 100}
    
    trained_model = trainModel(training_rdd, model_params)
    ```

#### 5.2 `predictData(model, inputData)`
- **描述**: 使用训练好的模型进行预测。
- **参数**:
  - `model`: 已训练的模型对象。
  - `inputData`: RDD，输入数据。
- **返回值**:
  - 成功时返回预测结果。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    input_data_rdd = ...
    
    predictions = predictData(trained_model, input_data_rdd)
    print(predictions)  # 输出预测结果或错误信息
    ```

#### 5.3 `evaluateModel(model, testData)`
- **描述**: 评估模型性能，返回评估结果。
- **参数**:
  - `model`: 已训练的模型对象。
  - `testData`: 测试数据（Brick 格式）。
- **返回值**:
  - 成功时返回评估结果。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    evaluation_results = evaluateModel(model, test_data)
    print(evaluation_results)  # 输出评估结果或错误信息
    ```

#### 5.4 `optimizeModel(model, optimizationParams)`
- **描述**: 根据优化参数调整和优化模型。
- **参数**

:
  - `model`: 已训练的模型对象。
  - `optimizationParams`: 字典，优化参数。
- **返回值**:
  - 成功时返回优化后的模型。
  - 失败时返回错误信息。
- **错误处理**:
  - 异常情况下返回错误信息并打印错误内容。
- **示例代码**:
    ```python
    optimized_model = optimizeModel(trained_model, {"maxIter": 50})
    ```
    

### **6. 算法 Brick 数据输入接口**

#### **1. Dataset_Custom 描述**
- `Dataset_Custom` 类继承自 PyTorch 的 `Dataset` 类，用于加载和处理自定义数据集。该类特别设计用于处理 `Brick` 格式的数据，并支持数据标准化、时间编码等功能。数据集可以按需划分为训练集、验证集和测试集，并支持多种时间序列相关的特性。

#### **2. 初始化方法**
```python
__init__(self, root_path, flag='train', size=None, features='S', data_path='brick_data.ttl', target='target', scale=True, timeenc=0, freq='h', seasonal_patterns=None)
```

- **描述**: 初始化 `Dataset_Custom` 对象，设置数据集的基础配置，如数据路径、分割类型、特征和目标变量的选择、标准化选项及时间编码方式。

- **参数**:
  
  - `root_path` (字符串): 数据集的根路径，用于定位数据文件。
  - `flag` (字符串): 数据集的分割标志，默认值为 `'train'`。可选值为 `'train'`、`'val'` 和 `'test'`，分别表示训练集、验证集和测试集。
  - `size` (列表或元组): 包含 `[seq_len, label_len, pred_len]` 的列表或元组，用于指定序列长度、标签长度和预测长度。如果未指定，将使用默认值。
  - `features` (字符串): 数据集中要使用的特征类型。默认值为 `'S'`，可以根据具体需求调整。
  - `data_path` (字符串): 数据文件的相对路径。默认值为 `'brick_data.ttl'`。
  - `target` (字符串): 目标特征的名称，默认为 `'target'`。
  - `scale` (布尔值): 是否对数据进行标准化，默认值为 `True`。
  - `timeenc` (整数): 时间编码类型，0 表示使用月份、日期、星期几和小时作为时间特征；1 表示使用 `time_features` 函数进行编码。默认值为 0。
  - `freq` (字符串): 数据的时间频率，例如 `'h'` 表示小时，`'d'` 表示天。默认值为 `'h'`。
  - `seasonal_patterns` (可选): 季节性模式的定义，未在此接口中具体实现。
  
- **返回值**: 
  - 无返回值。初始化方法创建并配置数据集对象，并加载和预处理数据。

- **错误处理**:
  - 无效的 `flag` 参数（即非 `'train'`、`'val'` 或 `'test'`）将触发 `AssertionError`，确保数据集类型正确。
  - 如果加载的数据为空，或数据路径无效，将抛出 `ValueError` 并提供详细的错误信息。

- **示例代码**:
  
    ```python
    dataset = Dataset_Custom(root_path='data/', flag='train', size=[96, 24, 24])
    print(f"Dataset length: {len(dataset)}")
    ```

#### **3. 主要方法**

##### **3.1 `__read_data__(self)`**

- **描述**: 读取并预处理数据。数据从指定的 TTL 文件中加载，并转换为 Pandas DataFrame 格式。根据 `flag` 参数将数据集分割为训练集、验证集或测试集，并应用标准化处理和时间特征编码。

- **返回值**: 无显式返回值，该方法内部更新类实例的属性。

- **错误处理**:
  - 在解析或加载数据时，如果数据为空或格式错误，将抛出 `ValueError`，提醒用户检查数据路径和查询条件。

- **示例代码**:
    ```python
    dataset.__read_data__()
    print("Data successfully loaded and preprocessed.")
    ```

##### **3.2 `__getitem__(self, index)`**

- **描述**: 获取指定索引位置的样本，包括输入序列、输出序列及其对应的时间戳。

- **参数**:
  - `index` (整数): 数据集中的索引位置，用于提取相应的数据片段。

- **返回值**:
  - 返回一个元组 `(seq_x, seq_y, seq_x_mark, seq_y_mark)`，其中包括输入序列、输出序列及其时间戳标记。

- **错误处理**: 
  - 方法内部未对索引进行显式检查，假设 `index` 在有效范围内。

- **示例代码**:
    ```python
    sample = dataset[0]
    print("Sample input sequence:", sample[0])
    print("Sample output sequence:", sample[1])
    ```

##### **3.3 `__len__(self)`**

- **描述**: 返回数据集的长度。

- **返回值**:
  - 返回数据集的有效样本数量，计算方法为 `len(self.data_x) - self.seq_len - self.pred_len + 1`。

- **错误处理**: 
  - 方法假设 `data_x` 已正确加载，未对数据长度进行异常处理。

- **示例代码**:
    ```python
    print(f"Total number of samples in the dataset: {len(dataset)}")
    ```

##### **3.4 `inverse_transform(self, data)`**

- **描述**: 对数据进行反标准化操作，将数据从标准化的状态还原为原始值。

- **参数**:
  - `data` (numpy 数组): 需要反标准化的数据。

- **返回值**:
  - 返回反标准化后的数据，与输入的形状相同。

- **错误处理**:
  - 如果 `data` 的形状或类型不匹配，将导致标准化器的操作失败。

- **示例代码**:
  
    ```python
    original_data = dataset.inverse_transform(scaled_data)
    print("Original data after inverse transform:", original_data)
    ```

### **7. CSV 转为 Brick 数据接口**

#### **1. 描述**
- 该代码旨在读取大型 CSV 文件，将其中的数据按照 `Brick` 架构转换为 RDF 图并分批次序列化为 Turtle (`.ttl`) 文件。为了提高处理速度，使用了 Python 的 `ThreadPoolExecutor` 进行多线程并行处理。

#### **2.1 `bind_namespaces(graph)`**

- **描述**: 为给定的 RDF 图绑定命名空间，使得在生成 RDF 数据时可以使用短名称代替完整的 URI，提高可读性。

- **参数**:
  
  - `graph` (`rdflib.Graph` 对象): 需要绑定命名空间的 RDF 图对象。
  
- **返回值**:
  - 返回绑定了命名空间的 `rdflib.Graph` 对象。

- **示例代码**:
    ```python
    g = bind_namespaces(Graph())
    ```

#### **2.2 `process_batch(batch_rows, batch_num)`**

- **描述**: 处理一批 CSV 数据，将其转换为 RDF 图，并将图序列化为 `.ttl` 文件。该函数被设计为多线程环境下的任务单元，每个线程处理一个批次的数据。

- **参数**:
  - `batch_rows` (列表): 包含一批 CSV 数据行的列表，每一行数据是一个字典。
  - `batch_num` (整数): 当前批次的编号，用于命名输出文件。

- **返回值**:
  - 返回当前批次的编号 `batch_num`，以便在主线程中确认任务完成。

- **错误处理**:
  - 如果序列化过程中出现异常，将捕获并输出错误信息，不影响其他批次的处理。

- **示例代码**:
    ```python
    batch_num = process_batch(batch_rows, 1)
    print(f"Batch {batch_num} processed.")
    ```

#### **2.3 主流程**

- **描述**: 
  
  1. 代码首先加载 `Brick 1.3` 的 TTL 文件，将其转换为 RDF 图，并为后续的 RDF 生成准备命名空间。
  2. 读取指定的 CSV 文件，并将数据按指定的批次大小进行分组存储。
  3. 使用 `ThreadPoolExecutor` 创建线程池，并为每一批次数据分配线程进行并行处理。每个线程调用 `process_batch` 函数生成 RDF 图并保存为 `.ttl` 文件。
  4. 所有批次完成后，主线程输出完成信息。
  
- **参数**:
  
  - `brick_file_path` (字符串): `Brick 1.3` TTL 文件的路径。
  - `csv_file_path` (字符串): 要读取的 CSV 文件的路径。
  - `batch_size` (整数): 每一批次处理的行数，默认值为 10,000 行。
  - `output_dir` (字符串): 序列化的 Turtle 文件存储目录，默认值为 `/home/yun/Public/Python/TimeMixer/ttl/`。
  
- **返回值**:
  - 无显式返回值，流程结束后输出所有批次处理完成的信息。

- **示例代码**:
    ```python
    # 设置批次大小和文件路径
    batch_size = 10000
    csv_file_path = '/path/to/csv_file.csv'
    
    # 执行主流程
    main()
    ```

- **错误处理**:
  - 在多线程环境下，每个线程可能会在处理过程中遇到异常，异常被捕获后，主线程将输出错误信息，但不会停止其他批次的处理。

#### **3. 运行示例**

- **示例代码**:
  
    ```python
    # 设置输入CSV文件和输出TTL文件的路径
    csv_file_path = '/home/yun/Public/Python/HVAC-Fault-Prediction/data/Data_Article_Dataset.csv'
    output_dir = '/home/yun/Public/Python/TimeMixer/ttl/'
    
    # 设置批次大小
    batch_size = 10000
    
    # 执行主流程
    main()
    ```
    
- **执行结果**:
    - 代码将CSV文件中的数据按批次处理，并将每一批次的数据保存为一个`.ttl`文件。所有批次完成后，会在终端输出所有批次均已完成的提示信息。

#### **4. 注意事项**
1. 批次大小（`batch_size`）的选择应基于系统内存和处理能力，过大的批次可能导致内存不足，过小的批次可能导致性能未充分利用。
2. 在高并发情况下，I/O 操作（例如文件写入）的性能可能会成为瓶颈，适当的线程数量和批次大小调节可以提高整体效率。

---

## 备注

1. 每个接口的示例代码均以Python编写，便于在实际开发中参考。
2. 所有接口均实现了异常处理，以提高代码的健壮性和可维护性。
3. 所有接口的输入参数需符合定义的格式，以确保接口的正确调用。
